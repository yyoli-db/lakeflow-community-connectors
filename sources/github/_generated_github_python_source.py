# ==============================================================================
# Merged Lakeflow Source: github
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/github/github.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the GitHub connector with connection-level options.

            Expected options:
                - token: Personal access token used for GitHub REST API authentication.
                - base_url (optional): Override for GitHub API base URL. Defaults to https://api.github.com.
            """
            token = options.get("token")
            if not token:
                raise ValueError("GitHub connector requires 'token' in options")

            self.base_url = options.get("base_url", "https://api.github.com").rstrip("/")

            # Configure a session with proper headers for GitHub REST API v3
            self._session = requests.Session()
            self._session.headers.update(
                {
                    "Authorization": f"Bearer {token}",
                    "Accept": "application/vnd.github+json",
                }
            )

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.

            """
            return [
                "issues",
                "repositories",
                "pull_requests",
                "comments",
                "commits",
                "assignees",
                "branches",
                "collaborators",
                "organizations",
                "teams",
                "users",
                "reviews",
            ]

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            The schema is static and derived from the GitHub REST API documentation
            and connector design for the `issues` object.
            """
            # Nested `user` / `assignee` struct schema
            user_struct = StructType(
                [
                    StructField("login", StringType(), True),
                    StructField("id", LongType(), True),
                    StructField("node_id", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("site_admin", BooleanType(), True),
                ]
            )

            # Nested `label` struct schema
            label_struct = StructType(
                [
                    StructField("id", LongType(), True),
                    StructField("node_id", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("color", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("default", BooleanType(), True),
                ]
            )

            # Nested `milestone` struct schema (subset of fields)
            milestone_struct = StructType(
                [
                    StructField("id", LongType(), True),
                    StructField("number", LongType(), True),
                    StructField("title", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("state", StringType(), True),
                    StructField("created_at", StringType(), True),
                    StructField("updated_at", StringType(), True),
                    StructField("due_on", StringType(), True),
                ]
            )

            # For `pull_request` and `reactions` we use MapType to keep the schema
            # flexible while preserving nested structure without flattening.
            pull_request_struct = MapType(StringType(), StringType(), True)
            reactions_struct = MapType(StringType(), StringType(), True)

            # Common nested structs used across multiple tables
            permissions_struct = StructType(
                [
                    StructField("admin", BooleanType(), True),
                    StructField("push", BooleanType(), True),
                    StructField("pull", BooleanType(), True),
                ]
            )

            license_struct = StructType(
                [
                    StructField("key", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("spdx_id", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("node_id", StringType(), True),
                ]
            )

            template_repository_struct = StructType(
                [
                    StructField("id", LongType(), True),
                    StructField("node_id", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("full_name", StringType(), True),
                ]
            )

            # Primary `issues` table schema
            issues_schema = StructType(
                [
                    StructField("id", LongType(), False),
                    StructField("node_id", StringType(), True),
                    StructField("number", LongType(), False),
                    StructField("repository_owner", StringType(), False),
                    StructField("repository_name", StringType(), False),
                    StructField("title", StringType(), True),
                    StructField("body", StringType(), True),
                    StructField("state", StringType(), True),
                    StructField("locked", BooleanType(), True),
                    StructField("comments", LongType(), True),
                    StructField("created_at", StringType(), True),
                    StructField("updated_at", StringType(), True),
                    StructField("closed_at", StringType(), True),
                    StructField("author_association", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("html_url", StringType(), True),
                    StructField("labels_url", StringType(), True),
                    StructField("comments_url", StringType(), True),
                    StructField("events_url", StringType(), True),
                    StructField("timeline_url", StringType(), True),
                    StructField("state_reason", StringType(), True),
                    StructField("user", user_struct, True),
                    StructField("assignee", user_struct, True),
                    StructField("assignees", ArrayType(user_struct, True), True),
                    StructField("labels", ArrayType(label_struct, True), True),
                    StructField("milestone", milestone_struct, True),
                    StructField("pull_request", pull_request_struct, True),
                    StructField("reactions", reactions_struct, True),
                ]
            )

            if table_name == "issues":
                return issues_schema

            # `repositories` snapshot metadata schema
            if table_name == "repositories":
                repositories_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("full_name", StringType(), True),
                        StructField("owner", user_struct, True),
                        StructField("private", BooleanType(), True),
                        StructField("html_url", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("fork", BooleanType(), True),
                        StructField("url", StringType(), True),
                        StructField("archive_url", StringType(), True),
                        StructField("assignees_url", StringType(), True),
                        StructField("blobs_url", StringType(), True),
                        StructField("branches_url", StringType(), True),
                        StructField("collaborators_url", StringType(), True),
                        StructField("comments_url", StringType(), True),
                        StructField("commits_url", StringType(), True),
                        StructField("compare_url", StringType(), True),
                        StructField("contents_url", StringType(), True),
                        StructField("contributors_url", StringType(), True),
                        StructField("deployments_url", StringType(), True),
                        StructField("downloads_url", StringType(), True),
                        StructField("events_url", StringType(), True),
                        StructField("forks_url", StringType(), True),
                        StructField("git_commits_url", StringType(), True),
                        StructField("git_refs_url", StringType(), True),
                        StructField("git_tags_url", StringType(), True),
                        StructField("git_url", StringType(), True),
                        StructField("issue_comment_url", StringType(), True),
                        StructField("issue_events_url", StringType(), True),
                        StructField("issues_url", StringType(), True),
                        StructField("keys_url", StringType(), True),
                        StructField("labels_url", StringType(), True),
                        StructField("languages_url", StringType(), True),
                        StructField("merges_url", StringType(), True),
                        StructField("milestones_url", StringType(), True),
                        StructField("notifications_url", StringType(), True),
                        StructField("pulls_url", StringType(), True),
                        StructField("releases_url", StringType(), True),
                        StructField("ssh_url", StringType(), True),
                        StructField("stargazers_url", StringType(), True),
                        StructField("statuses_url", StringType(), True),
                        StructField("subscribers_url", StringType(), True),
                        StructField("subscription_url", StringType(), True),
                        StructField("tags_url", StringType(), True),
                        StructField("teams_url", StringType(), True),
                        StructField("trees_url", StringType(), True),
                        StructField("clone_url", StringType(), True),
                        StructField("mirror_url", StringType(), True),
                        StructField("hooks_url", StringType(), True),
                        StructField("svn_url", StringType(), True),
                        StructField("homepage", StringType(), True),
                        StructField("language", StringType(), True),
                        StructField("forks_count", LongType(), True),
                        StructField("stargazers_count", LongType(), True),
                        StructField("watchers_count", LongType(), True),
                        StructField("size", LongType(), True),
                        StructField("default_branch", StringType(), True),
                        StructField("open_issues_count", LongType(), True),
                        StructField("is_template", BooleanType(), True),
                        StructField("topics", ArrayType(StringType(), True), True),
                        StructField("has_issues", BooleanType(), True),
                        StructField("has_projects", BooleanType(), True),
                        StructField("has_wiki", BooleanType(), True),
                        StructField("has_pages", BooleanType(), True),
                        StructField("has_downloads", BooleanType(), True),
                        StructField("archived", BooleanType(), True),
                        StructField("disabled", BooleanType(), True),
                        StructField("visibility", StringType(), True),
                        StructField("pushed_at", StringType(), True),
                        StructField("created_at", StringType(), True),
                        StructField("updated_at", StringType(), True),
                        StructField("permissions", permissions_struct, True),
                        StructField("allow_rebase_merge", BooleanType(), True),
                        StructField(
                            "template_repository", template_repository_struct, True
                        ),
                        StructField("temp_clone_token", StringType(), True),
                        StructField("allow_squash_merge", BooleanType(), True),
                        StructField("allow_merge_commit", BooleanType(), True),
                        StructField("subscribers_count", LongType(), True),
                        StructField("network_count", LongType(), True),
                        StructField("license", license_struct, True),
                    ]
                )
                return repositories_schema

            # `pull_requests` cdc table schema
            if table_name == "pull_requests":
                pull_requests_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("number", LongType(), False),
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("state", StringType(), True),
                        StructField("title", StringType(), True),
                        StructField("body", StringType(), True),
                        StructField("draft", BooleanType(), True),
                        StructField("created_at", StringType(), True),
                        StructField("updated_at", StringType(), True),
                        StructField("closed_at", StringType(), True),
                        StructField("merged_at", StringType(), True),
                        StructField("merge_commit_sha", StringType(), True),
                        StructField("user", user_struct, True),
                        StructField(
                            "base", MapType(StringType(), StringType(), True), True
                        ),
                        StructField(
                            "head", MapType(StringType(), StringType(), True), True
                        ),
                        StructField("html_url", StringType(), True),
                        StructField("url", StringType(), True),
                    ]
                )
                return pull_requests_schema

            # `comments` cdc table schema (issue comments)
            if table_name == "comments":
                comments_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("issue_url", StringType(), True),
                        StructField("html_url", StringType(), True),
                        StructField("body", StringType(), True),
                        StructField("user", user_struct, True),
                        StructField("created_at", StringType(), True),
                        StructField("updated_at", StringType(), True),
                        StructField("author_association", StringType(), True),
                    ]
                )
                return comments_schema

            # `commits` append-only table schema
            if table_name == "commits":
                commits_schema = StructType(
                    [
                        StructField("sha", StringType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("commit_message", StringType(), True),
                        StructField("commit_author_name", StringType(), True),
                        StructField("commit_author_email", StringType(), True),
                        StructField("commit_author_date", StringType(), True),
                        StructField("commit_committer_name", StringType(), True),
                        StructField("commit_committer_email", StringType(), True),
                        StructField("commit_committer_date", StringType(), True),
                        StructField("html_url", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("author", user_struct, True),
                        StructField("committer", user_struct, True),
                    ]
                )
                return commits_schema

            # `users` snapshot metadata schema
            if table_name == "users":
                users_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("login", StringType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("type", StringType(), True),
                        StructField("site_admin", BooleanType(), True),
                        StructField("name", StringType(), True),
                        StructField("company", StringType(), True),
                        StructField("blog", StringType(), True),
                        StructField("location", StringType(), True),
                        StructField("email", StringType(), True),
                        StructField("created_at", StringType(), True),
                        StructField("updated_at", StringType(), True),
                    ]
                )
                return users_schema

            # `organizations` snapshot metadata schema
            if table_name == "organizations":
                # The organizations table now reflects the shape of the
                # `GET /user/orgs` response without expanding each org via
                # `GET /orgs/{org}`. This avoids additional permissions that
                # may be required for the detail endpoint while still exposing
                # the most useful summary metadata.
                orgs_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("login", StringType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("repos_url", StringType(), True),
                        StructField("events_url", StringType(), True),
                        StructField("hooks_url", StringType(), True),
                        StructField("issues_url", StringType(), True),
                        StructField("members_url", StringType(), True),
                        StructField("public_members_url", StringType(), True),
                        StructField("avatar_url", StringType(), True),
                        StructField("description", StringType(), True),
                    ]
                )
                return orgs_schema

            # `teams` snapshot metadata schema
            if table_name == "teams":
                teams_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("organization_login", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("slug", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("privacy", StringType(), True),
                        StructField("permission", StringType(), True),
                    ]
                )
                return teams_schema

            # `assignees` snapshot metadata schema
            if table_name == "assignees":
                assignees_schema = StructType(
                    [
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("login", StringType(), False),
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("type", StringType(), True),
                        StructField("site_admin", BooleanType(), True),
                    ]
                )
                return assignees_schema

            # `collaborators` snapshot metadata schema
            if table_name == "collaborators":
                collaborators_schema = StructType(
                    [
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("login", StringType(), False),
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("type", StringType(), True),
                        StructField("site_admin", BooleanType(), True),
                        StructField("permissions", permissions_struct, True),
                    ]
                )
                return collaborators_schema

            # `branches` snapshot metadata schema
            if table_name == "branches":
                commit_ref_struct = StructType(
                    [
                        StructField("sha", StringType(), True),
                        StructField("url", StringType(), True),
                    ]
                )
                branches_schema = StructType(
                    [
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("name", StringType(), False),
                        StructField("commit", commit_ref_struct, True),
                        StructField("protected", BooleanType(), True),
                        StructField("protection_url", StringType(), True),
                    ]
                )
                return branches_schema

            # `reviews` append-only table schema
            if table_name == "reviews":
                reviews_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("node_id", StringType(), True),
                        StructField("repository_owner", StringType(), False),
                        StructField("repository_name", StringType(), False),
                        StructField("pull_number", LongType(), False),
                        StructField("state", StringType(), True),
                        StructField("body", StringType(), True),
                        StructField("user", user_struct, True),
                        StructField("commit_id", StringType(), True),
                        StructField("submitted_at", StringType(), True),
                        StructField("html_url", StringType(), True),
                    ]
                )
                return reviews_schema

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.

            For `issues`:
                - ingestion_type: cdc
                - primary_keys: ["id"]
                - cursor_field: updated_at
            """
            if table_name == "issues":
                return {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                }
            if table_name == "repositories":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "pull_requests":
                return {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                }
            if table_name == "comments":
                return {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                }
            if table_name == "commits":
                # Append-only stream keyed by immutable sha
                return {
                    "primary_keys": ["sha"],
                    "ingestion_type": "append",
                }
            if table_name == "users":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "organizations":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "teams":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "assignees":
                return {
                    "primary_keys": ["repository_owner", "repository_name", "id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "collaborators":
                return {
                    "primary_keys": ["repository_owner", "repository_name", "id"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "branches":
                return {
                    "primary_keys": ["repository_owner", "repository_name", "name"],
                    "ingestion_type": "snapshot",
                }
            if table_name == "reviews":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "append",
                }

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table and return raw JSON-like dictionaries.

            For the `issues` table this method:
                - Uses `/repos/{owner}/{repo}/issues` endpoint.
                - Supports incremental reads via the `since` query parameter mapped to `updated_at`.
                - Paginates using GitHub's `Link` header until all pages are read or
                  a batch limit (if provided via table_options) is reached.

            Other tables follow similar patterns based on their documented endpoints.

            Required table_options for `issues`:
                - owner: Repository owner (user or organization).
                - repo: Repository name.

            Optional table_options:
                - state: Issue state filter (default: "all").
                - per_page: Page size (max 100, default 100).
                - start_date: Initial ISO 8601 timestamp for first run if no start_offset is provided.
                - lookback_seconds: Lookback window applied when computing next cursor (default: 300).
                - max_pages_per_batch: Optional safety limit on pages per read_table call.
            """
            if table_name == "issues":
                return self._read_issues(start_offset, table_options)
            if table_name == "repositories":
                return self._read_repositories(start_offset, table_options)
            if table_name == "pull_requests":
                return self._read_pull_requests(start_offset, table_options)
            if table_name == "comments":
                return self._read_comments(start_offset, table_options)
            if table_name == "commits":
                return self._read_commits(start_offset, table_options)
            if table_name == "assignees":
                return self._read_assignees(start_offset, table_options)
            if table_name == "branches":
                return self._read_branches(start_offset, table_options)
            if table_name == "collaborators":
                return self._read_collaborators(start_offset, table_options)
            if table_name == "organizations":
                return self._read_organizations(start_offset, table_options)
            if table_name == "teams":
                return self._read_teams(start_offset, table_options)
            if table_name == "users":
                return self._read_users(start_offset, table_options)
            if table_name == "reviews":
                return self._read_reviews(start_offset, table_options)

            raise ValueError(f"Unsupported table: {table_name!r}")

        def _read_issues(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """Internal implementation for reading the `issues` table."""
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'issues' must include non-empty 'owner' and 'repo'"
                )

            state = table_options.get("state", "all")

            # Page size and safety limits
            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            try:
                lookback_seconds = int(table_options.get("lookback_seconds", 300))
            except (TypeError, ValueError):
                lookback_seconds = 300

            # Determine the starting cursor (ISO 8601 string)
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            # Build initial request
            url = f"{self.base_url}/repos/{owner}/{repo}/issues"
            params = {
                "state": state,
                "per_page": per_page,
                "sort": "updated",
                "direction": "asc",
            }
            if cursor:
                params["since"] = cursor

            records: list[dict[str, Any]] = []
            max_updated_at: str | None = None

            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for issues: {response.status_code} {response.text}"
                    )

                issues = response.json() or []
                if not isinstance(issues, list):
                    raise ValueError(
                        f"Unexpected response format for issues: {type(issues).__name__}"
                    )

                for issue in issues:
                    # Shallow-copy the raw JSON and add connector-derived fields.
                    record: dict[str, Any] = dict(issue)
                    record["repository_owner"] = owner
                    record["repository_name"] = repo
                    records.append(record)

                    updated_at = record.get("updated_at")
                    if isinstance(updated_at, str):
                        if max_updated_at is None or updated_at > max_updated_at:
                            max_updated_at = updated_at

                # Handle pagination via Link header
                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                # Subsequent requests follow the next URL as provided (no extra params)
                next_url = next_link
                next_params = None
                pages_fetched += 1

            # Compute the next cursor with a small lookback window to avoid missing records
            next_cursor = cursor
            if max_updated_at:
                try:
                    dt = datetime.strptime(max_updated_at, "%Y-%m-%dT%H:%M:%SZ")
                    dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
                    next_cursor = dt_with_lookback.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    # Fallback: if parsing fails, just reuse the raw max_updated_at
                    next_cursor = max_updated_at

            # If no new records, return the same offset to indicate end of stream for this batch
            if not records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor} if next_cursor else {}

            return iter(records), next_offset

        def _read_repositories(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `repositories` snapshot table.

            This implementation lists repositories for a given user or organization
            using the GitHub REST API:

                - GET /users/{username}/repos
                - GET /orgs/{org}/repos

            The returned JSON objects already have the full repository shape described
            in the connector schema. We add connector-derived fields:
                - repository_owner: owner.login
                - repository_name: name

            Required table_options:
                - Either:
                  - owner: GitHub username (for /users/{username}/repos)
                  - or org: GitHub organization login (for /orgs/{org}/repos)

            Optional table_options:
                - per_page: Page size (max 100, default 100).
                - max_pages_per_batch: Optional safety limit on pages per read_table call.
            """
            owner = table_options.get("owner")
            org = table_options.get("org")

            if owner and org:
                raise ValueError(
                    "table_configuration for 'repositories' must not include both 'owner' and 'org'; "
                    "specify only one."
                )
            if not owner and not org:
                raise ValueError(
                    "table_configuration for 'repositories' must include either 'owner' (username) "
                    "or 'org' (organization login)"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            if org:
                url = f"{self.base_url}/orgs/{org}/repos"
            else:
                url = f"{self.base_url}/users/{owner}/repos"

            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for repositories: {response.status_code} {response.text}"
                    )

                repos = response.json() or []
                if not isinstance(repos, list):
                    raise ValueError(
                        f"Unexpected response format for repositories: {type(repos).__name__}"
                    )

                for repo_obj in repos:
                    record: dict[str, Any] = dict(repo_obj)
                    owner_obj = repo_obj.get("owner") or {}
                    record["repository_owner"] = owner_obj.get("login")
                    record["repository_name"] = repo_obj.get("name")
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            return iter(records), {}

        def _read_pull_requests(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pull_requests` cdc table using:
                GET /repos/{owner}/{repo}/pulls

            Incremental behaviour mirrors issues using updated_at as a cursor,
            but for now this implementation always performs a forward read
            from the provided (optional) cursor.
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'pull_requests' must include non-empty 'owner' and 'repo'"
                )

            state = table_options.get("state", "all")

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            try:
                lookback_seconds = int(table_options.get("lookback_seconds", 300))
            except (TypeError, ValueError):
                lookback_seconds = 300

            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            url = f"{self.base_url}/repos/{owner}/{repo}/pulls"
            params = {
                "state": state,
                "per_page": per_page,
                "sort": "updated",
                "direction": "asc",
            }
            if cursor:
                params["since"] = cursor

            records: list[dict[str, Any]] = []
            max_updated_at: str | None = None

            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for pull_requests: {response.status_code} {response.text}"
                    )

                pull_requests = response.json() or []
                if not isinstance(pull_requests, list):
                    raise ValueError(
                        f"Unexpected response format for pull_requests: {type(pull_requests).__name__}"
                    )

                for pr in pull_requests:
                    record: dict[str, Any] = dict(pr)
                    record["repository_owner"] = owner
                    record["repository_name"] = repo
                    records.append(record)

                    updated_at = record.get("updated_at")
                    if isinstance(updated_at, str):
                        if max_updated_at is None or updated_at > max_updated_at:
                            max_updated_at = updated_at

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            next_cursor = cursor
            if max_updated_at:
                try:
                    dt = datetime.strptime(max_updated_at, "%Y-%m-%dT%H:%M:%SZ")
                    dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
                    next_cursor = dt_with_lookback.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    next_cursor = max_updated_at

            if not records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor} if next_cursor else {}

            return iter(records), next_offset

        def _read_comments(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `comments` cdc table using:
                GET /repos/{owner}/{repo}/issues/comments
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'comments' must include non-empty 'owner' and 'repo'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            try:
                lookback_seconds = int(table_options.get("lookback_seconds", 300))
            except (TypeError, ValueError):
                lookback_seconds = 300

            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            url = f"{self.base_url}/repos/{owner}/{repo}/issues/comments"
            params = {
                "per_page": per_page,
                "sort": "updated",
                "direction": "asc",
            }
            if cursor:
                params["since"] = cursor

            records: list[dict[str, Any]] = []
            max_updated_at: str | None = None

            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for comments: {response.status_code} {response.text}"
                    )

                comments = response.json() or []
                if not isinstance(comments, list):
                    raise ValueError(
                        f"Unexpected response format for comments: {type(comments).__name__}"
                    )

                for comment in comments:
                    record: dict[str, Any] = dict(comment)
                    record["repository_owner"] = owner
                    record["repository_name"] = repo
                    records.append(record)

                    updated_at = record.get("updated_at")
                    if isinstance(updated_at, str):
                        if max_updated_at is None or updated_at > max_updated_at:
                            max_updated_at = updated_at

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            next_cursor = cursor
            if max_updated_at:
                try:
                    dt = datetime.strptime(max_updated_at, "%Y-%m-%dT%H:%M:%SZ")
                    dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
                    next_cursor = dt_with_lookback.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    next_cursor = max_updated_at

            if not records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor} if next_cursor else {}

            return iter(records), next_offset

        def _read_commits(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `commits` append-only table using:
                GET /repos/{owner}/{repo}/commits
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'commits' must include non-empty 'owner' and 'repo'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            url = f"{self.base_url}/repos/{owner}/{repo}/commits"
            params = {
                "per_page": per_page,
            }
            if cursor:
                params["since"] = cursor

            records: list[dict[str, Any]] = []
            max_commit_date: str | None = None

            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for commits: {response.status_code} {response.text}"
                    )

                commits = response.json() or []
                if not isinstance(commits, list):
                    raise ValueError(
                        f"Unexpected response format for commits: {type(commits).__name__}"
                    )

                for commit_obj in commits:
                    commit_info = commit_obj.get("commit", {}) or {}
                    commit_author = commit_info.get("author", {}) or {}
                    commit_committer = commit_info.get("committer", {}) or {}

                    record: dict[str, Any] = {
                        "sha": commit_obj.get("sha"),
                        "node_id": commit_obj.get("node_id"),
                        "repository_owner": owner,
                        "repository_name": repo,
                        "commit_message": commit_info.get("message"),
                        "commit_author_name": commit_author.get("name"),
                        "commit_author_email": commit_author.get("email"),
                        "commit_author_date": commit_author.get("date"),
                        "commit_committer_name": commit_committer.get("name"),
                        "commit_committer_email": commit_committer.get("email"),
                        "commit_committer_date": commit_committer.get("date"),
                        "html_url": commit_obj.get("html_url"),
                        "url": commit_obj.get("url"),
                        "author": commit_obj.get("author"),
                        "committer": commit_obj.get("committer"),
                    }
                    records.append(record)

                    author_date = record.get("commit_author_date")
                    if isinstance(author_date, str):
                        if max_commit_date is None or author_date > max_commit_date:
                            max_commit_date = author_date

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            next_cursor = cursor
            if max_commit_date:
                # For commits we simply reuse the max commit author date as the next cursor.
                next_cursor = max_commit_date

            if not records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor} if next_cursor else {}

            return iter(records), next_offset

        def _read_assignees(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `assignees` snapshot table using:
                GET /repos/{owner}/{repo}/assignees
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'assignees' must include non-empty 'owner' and 'repo'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            url = f"{self.base_url}/repos/{owner}/{repo}/assignees"
            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for assignees: {response.status_code} {response.text}"
                    )

                assignees = response.json() or []
                if not isinstance(assignees, list):
                    raise ValueError(
                        f"Unexpected response format for assignees: {type(assignees).__name__}"
                    )

                for assignee in assignees:
                    record: dict[str, Any] = {
                        "repository_owner": owner,
                        "repository_name": repo,
                        "login": assignee.get("login"),
                        "id": assignee.get("id"),
                        "node_id": assignee.get("node_id"),
                        "type": assignee.get("type"),
                        "site_admin": assignee.get("site_admin"),
                    }
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            return iter(records), {}

        def _read_branches(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `branches` snapshot table using:
                GET /repos/{owner}/{repo}/branches
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'branches' must include non-empty 'owner' and 'repo'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            url = f"{self.base_url}/repos/{owner}/{repo}/branches"
            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for branches: {response.status_code} {response.text}"
                    )

                branches = response.json() or []
                if not isinstance(branches, list):
                    raise ValueError(
                        f"Unexpected response format for branches: {type(branches).__name__}"
                    )

                for branch in branches:
                    record: dict[str, Any] = {
                        "repository_owner": owner,
                        "repository_name": repo,
                        "name": branch.get("name"),
                        "commit": branch.get("commit"),
                        "protected": branch.get("protected"),
                        "protection_url": branch.get("protection_url"),
                    }
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            return iter(records), {}

        def _read_collaborators(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `collaborators` snapshot table using:
                GET /repos/{owner}/{repo}/collaborators
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'collaborators' must include non-empty 'owner' and 'repo'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            url = f"{self.base_url}/repos/{owner}/{repo}/collaborators"
            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for collaborators: {response.status_code} {response.text}"
                    )

                collaborators = response.json() or []
                if not isinstance(collaborators, list):
                    raise ValueError(
                        f"Unexpected response format for collaborators: {type(collaborators).__name__}"
                    )

                for collaborator in collaborators:
                    record: dict[str, Any] = {
                        "repository_owner": owner,
                        "repository_name": repo,
                        "login": collaborator.get("login"),
                        "id": collaborator.get("id"),
                        "node_id": collaborator.get("node_id"),
                        "type": collaborator.get("type"),
                        "site_admin": collaborator.get("site_admin"),
                        "permissions": collaborator.get("permissions"),
                    }
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            return iter(records), {}

        def _read_organizations(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `organizations` snapshot table.

            Instead of requiring an explicit `org` option, this method discovers
            organizations for the authenticated user using:

                - GET /user/orgs                  (list orgs the token can see)

            It intentionally does **not** expand each organization via
            `GET /orgs/{org}` to avoid additional permission requirements on
            the detail endpoint. The table therefore exposes the summary
            metadata returned directly by `GET /user/orgs`.
            """
            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            # List organizations visible to the authenticated user
            url = f"{self.base_url}/user/orgs"
            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for organizations: {response.status_code} {response.text}"
                    )

                orgs = response.json() or []
                if not isinstance(orgs, list):
                    raise ValueError(
                        f"Unexpected response format for organizations list: {type(orgs).__name__}"
                    )

                # Use the summary objects returned directly by GET /user/orgs
                # without further expansion. Map the response into the schema
                # defined in `get_table_schema`.
                for org_summary in orgs:
                    if not isinstance(org_summary, dict):
                        # Skip malformed entries instead of failing the whole batch
                        continue

                    record: dict[str, Any] = {
                        "id": org_summary.get("id"),
                        "login": org_summary.get("login"),
                        "node_id": org_summary.get("node_id"),
                        "url": org_summary.get("url"),
                        "repos_url": org_summary.get("repos_url"),
                        "events_url": org_summary.get("events_url"),
                        "hooks_url": org_summary.get("hooks_url"),
                        "issues_url": org_summary.get("issues_url"),
                        "members_url": org_summary.get("members_url"),
                        "public_members_url": org_summary.get("public_members_url"),
                        "avatar_url": org_summary.get("avatar_url"),
                        "description": org_summary.get("description"),
                    }
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            # Snapshot table  no incremental cursor at the moment.
            return iter(records), {}

        def _read_teams(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `teams` snapshot table.

            Instead of requiring an explicit `org` option, this method discovers
            teams for the authenticated user using:

                - GET /user/teams                          (list teams user can see)
                - GET /orgs/{org}/teams/{team_slug}        (expand each team)

            The connector also adds `organization_login` to each record to match
            the declared schema.
            """
            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            # List teams visible to the authenticated user
            url = f"{self.base_url}/user/teams"
            params = {"per_page": per_page}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for teams: {response.status_code} {response.text}"
                    )

                teams = response.json() or []
                if not isinstance(teams, list):
                    raise ValueError(
                        f"Unexpected response format for teams list: {type(teams).__name__}"
                    )

                # Expand each team via GET /orgs/{org}/teams/{team_slug}
                for team_summary in teams:
                    org_obj = team_summary.get("organization") or {}
                    org_login = org_obj.get("login")
                    team_slug = team_summary.get("slug")
                    if not org_login or not team_slug:
                        # Skip malformed entries instead of failing the whole batch
                        continue

                    detail_url = f"{self.base_url}/orgs/{org_login}/teams/{team_slug}"
                    detail_resp = self._session.get(detail_url, timeout=30)
                    if detail_resp.status_code != 200:
                        raise RuntimeError(
                            "GitHub API error for team "
                            f"{org_login!r}/{team_slug!r}: "
                            f"{detail_resp.status_code} {detail_resp.text}"
                        )

                    team_obj = detail_resp.json() or {}
                    if not isinstance(team_obj, dict):
                        raise ValueError(
                            "Unexpected response format for team detail: "
                            f"{type(team_obj).__name__}"
                        )

                    record: dict[str, Any] = dict(team_obj)
                    record["organization_login"] = org_login
                    records.append(record)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            # Snapshot table  no incremental cursor at the moment.
            return iter(records), {}

        def _read_users(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `users` snapshot table using:
                GET /user

            The connector now resolves the user from the authenticated context and
            no longer requires a `username` option. This returns metadata for the
            current authenticated user.
            """
            url = f"{self.base_url}/user"
            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"GitHub API error for users: {response.status_code} {response.text}"
                )

            user_obj = response.json() or {}
            if not isinstance(user_obj, dict):
                raise ValueError(
                    f"Unexpected response format for user: {type(user_obj).__name__}"
                )

            record: dict[str, Any] = dict(user_obj)
            return iter([record]), {}

        def _read_reviews(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `reviews` append-only table.

            Primary child API:
                - GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews

            Parent listing when pull_number is not provided (see Step 3 guidance in
            the connector coding instructions about parent/child relationships):
                - GET /repos/{owner}/{repo}/pulls
                  Then for each pull request, call the reviews API above and
                  combine all reviews into a single logical table.
            """
            owner = table_options.get("owner")
            repo = table_options.get("repo")
            if not owner or not repo:
                raise ValueError(
                    "table_configuration for 'reviews' must include non-empty 'owner' and 'repo'"
                )

            # Page size and safety limits for listing parent pull requests and
            # paginating through reviews.
            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages_per_batch = int(table_options.get("max_pages_per_batch", 50))
            except (TypeError, ValueError):
                max_pages_per_batch = 50

            pull_number_opt = table_options.get("pull_number")

            records: list[dict[str, Any]] = []

            def _fetch_reviews_for_pull(pull_number: int) -> None:
                """
                Fetch reviews for a single pull request and append them to `records`.
                This uses the list API and respects pagination, combining all pages.
                """
                url = f"{self.base_url}/repos/{owner}/{repo}/pulls/{pull_number}/reviews"
                params = {"per_page": per_page}

                pages_fetched = 0
                next_url: str | None = url
                next_params = params

                while next_url and pages_fetched < max_pages_per_batch:
                    response = self._session.get(next_url, params=next_params, timeout=30)
                    if response.status_code != 200:
                        raise RuntimeError(
                            "GitHub API error for reviews "
                            f"for pull_request {pull_number}: "
                            f"{response.status_code} {response.text}"
                        )

                    reviews = response.json() or []
                    if not isinstance(reviews, list):
                        raise ValueError(
                            "Unexpected response format for reviews: "
                            f"{type(reviews).__name__}"
                        )

                    for review in reviews:
                        record: dict[str, Any] = dict(review)
                        record["repository_owner"] = owner
                        record["repository_name"] = repo
                        record["pull_number"] = int(pull_number)
                        records.append(record)

                    link_header = response.headers.get("Link", "")
                    next_link = self._extract_next_link(link_header)
                    if not next_link:
                        break

                    next_url = next_link
                    next_params = None
                    pages_fetched += 1

            # If a specific pull_number is provided, preserve the original behavior
            # while now also supporting pagination of the reviews list.
            if pull_number_opt is not None:
                try:
                    pull_number_int = int(pull_number_opt)
                except (TypeError, ValueError):
                    raise ValueError(
                        f"table_options['pull_number'] must be an int-compatible value, "
                        f"got {pull_number_opt!r}"
                    )

                _fetch_reviews_for_pull(pull_number_int)
                return iter(records), {}

            # If no specific pull_number is provided, list all pull requests for the
            # repository and fetch reviews for each, combining them into a single
            # logical table. This follows the recommended pattern for child objects
            # when parent identifiers are optional (see Step 3 of the coding guide).
            pr_state = table_options.get("state", "all")
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls"
            params = {
                "state": pr_state,
                "per_page": per_page,
            }

            pages_fetched = 0
            next_url: str | None = url
            next_params = params

            while next_url and pages_fetched < max_pages_per_batch:
                response = self._session.get(next_url, params=next_params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"GitHub API error for pull_requests while discovering reviews: "
                        f"{response.status_code} {response.text}"
                    )

                pull_requests = response.json() or []
                if not isinstance(pull_requests, list):
                    raise ValueError(
                        "Unexpected response format for pull_requests when discovering "
                        f"reviews: {type(pull_requests).__name__}"
                    )

                for pr in pull_requests:
                    number = pr.get("number")
                    if isinstance(number, int):
                        _fetch_reviews_for_pull(number)

                link_header = response.headers.get("Link", "")
                next_link = self._extract_next_link(link_header)
                if not next_link:
                    break

                next_url = next_link
                next_params = None
                pages_fetched += 1

            # Snapshot append-only table  we currently do not track an incremental
            # cursor for reviews, so the offset is always an empty dict.
            return iter(records), {}

        @staticmethod
        def _extract_next_link(link_header: str | None) -> str | None:
            """
            Parse the GitHub Link header to extract the URL with rel="next".
            """
            if not link_header:
                return None

            parts = link_header.split(",")
            for part in parts:
                section = part.strip()
                if 'rel="next"' in section:
                    # Format: <url>; rel="next"
                    start = section.find("<")
                    end = section.find(">", start + 1)
                    if start != -1 and end != -1:
                        return section[start + 1 : end]
            return None


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
