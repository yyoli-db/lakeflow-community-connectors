# ==============================================================================
# Merged Lakeflow Source: zendesk
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)

from locale import dcgettext
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type
                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/zendesk/zendesk.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict) -> None:
            self.subdomain = options["subdomain"]
            self.email = options["email"]
            self.api_token = options["api_token"]
            self.base_url = f"https://{self.subdomain}.zendesk.com/api/v2"
            user = f"{self.email}/token"
            token = self.api_token
            auth_str = f"{user}:{token}"
            self.auth_header = {
                "Authorization": "Basic " + base64.b64encode(auth_str.encode()).decode(),
                "Content-Type": "application/json",
            }

        def list_tables(self) -> List[str]:
            return [
                "tickets",
                "organizations",
                "articles",
                "brands",
                "groups",
                "ticket_comments",
                "topics",
                "users",
            ]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            """
            schemas = {
                "tickets": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("external_id", StringType()),
                        StructField("via", MapType(StringType(), StringType())),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("type", StringType()),
                        StructField("subject", StringType()),
                        StructField("raw_subject", StringType()),
                        StructField("description", StringType()),
                        StructField("priority", StringType()),
                        StructField("status", StringType()),
                        StructField("recipient", StringType()),
                        StructField("requester_id", LongType()),
                        StructField("submitter_id", LongType()),
                        StructField("assignee_id", LongType()),
                        StructField("organization_id", LongType()),
                        StructField("group_id", LongType()),
                        StructField("collaborator_ids", ArrayType(LongType())),
                        StructField("follower_ids", ArrayType(LongType())),
                        StructField("email_cc_ids", ArrayType(LongType())),
                        StructField("forum_topic_id", LongType()),
                        StructField("problem_id", LongType()),
                        StructField("has_incidents", BooleanType()),
                        StructField("is_public", BooleanType()),
                        StructField("due_at", StringType()),
                        StructField("tags", ArrayType(StringType())),
                        StructField(
                            "custom_fields",
                            ArrayType(MapType(StringType(), StringType())),
                        ),
                        StructField(
                            "satisfaction_rating", MapType(StringType(), StringType())
                        ),
                        StructField("sharing_agreement_ids", ArrayType(LongType())),
                        StructField(
                            "fields", ArrayType(MapType(StringType(), StringType()))
                        ),
                        StructField("followup_ids", ArrayType(LongType())),
                        StructField("ticket_form_id", LongType()),
                        StructField("brand_id", LongType()),
                        StructField("allow_channelback", BooleanType()),
                        StructField("allow_attachments", BooleanType()),
                        StructField("from_messaging_channel", BooleanType()),
                        StructField("generated_timestamp", LongType()),
                    ]
                ),
                "organizations": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("external_id", StringType()),
                        StructField("name", StringType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("domain_names", ArrayType(StringType())),
                        StructField("details", StringType()),
                        StructField("notes", StringType()),
                        StructField("group_id", LongType()),
                        StructField("shared_tickets", BooleanType()),
                        StructField("shared_comments", BooleanType()),
                        StructField("tags", ArrayType(StringType())),
                        StructField(
                            "organization_fields", MapType(StringType(), StringType())
                        ),
                    ]
                ),
                "articles": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("html_url", StringType()),
                        StructField("author_id", LongType()),
                        StructField("comments_disabled", BooleanType()),
                        StructField("draft", BooleanType()),
                        StructField("promoted", BooleanType()),
                        StructField("position", LongType()),
                        StructField("vote_sum", LongType()),
                        StructField("vote_count", LongType()),
                        StructField("section_id", LongType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("name", StringType()),
                        StructField("title", StringType()),
                        StructField("source_locale", StringType()),
                        StructField("locale", StringType()),
                        StructField("outdated", BooleanType()),
                        StructField("outdated_locales", ArrayType(StringType())),
                        StructField("edited_at", StringType()),
                        StructField("user_segment_id", LongType()),
                        StructField("permission_group_id", LongType()),
                        StructField("content_tag_ids", ArrayType(LongType())),
                        StructField("label_names", ArrayType(StringType())),
                        StructField("body", StringType()),
                    ]
                ),
                "brands": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("name", StringType()),
                        StructField("brand_url", StringType()),
                        StructField("subdomain", StringType()),
                        StructField("host_mapping", StringType()),
                        StructField("has_help_center", BooleanType()),
                        StructField("help_center_state", StringType()),
                        StructField("active", BooleanType()),
                        StructField("default", BooleanType()),
                        StructField("is_deleted", BooleanType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("ticket_form_ids", ArrayType(LongType())),
                        StructField("signature_template", StringType()),
                    ]
                ),
                "groups": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("name", StringType()),
                        StructField("deleted", BooleanType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("description", StringType()),
                        StructField("default", BooleanType()),
                        StructField("is_public", BooleanType()),
                    ]
                ),
                "ticket_comments": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("type", StringType()),
                        StructField("request_id", LongType()),
                        StructField("requester_id", LongType()),
                        StructField("status", StringType()),
                        StructField("subject", StringType()),
                        StructField("priority", StringType()),
                        StructField("organization_id", LongType()),
                        StructField("description", StringType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("assignee_id", LongType()),
                        StructField("group_id", LongType()),
                        StructField("collaborator_ids", ArrayType(LongType())),
                        StructField(
                            "custom_fields",
                            ArrayType(MapType(StringType(), StringType())),
                        ),
                        StructField("email_cc_ids", ArrayType(LongType())),
                        StructField("follower_ids", ArrayType(LongType())),
                        StructField("ticket_form_id", LongType()),
                        StructField("brand_id", LongType()),
                        StructField(
                            "comments", ArrayType(MapType(StringType(), StringType()))
                        ),
                    ]
                ),
                "topics": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("html_url", StringType()),
                        StructField("name", StringType()),
                        StructField("description", StringType()),
                        StructField("position", LongType()),
                        StructField("follower_count", LongType()),
                        StructField("community_id", LongType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("user_segment_id", LongType()),
                        StructField("manageable_by", StringType()),
                        StructField("user_segment_ids", ArrayType(LongType())),
                    ]
                ),
                "users": StructType(
                    [
                        StructField("id", LongType()),
                        StructField("url", StringType()),
                        StructField("email", StringType()),
                        StructField("name", StringType()),
                        StructField("active", BooleanType()),
                        StructField("alias", StringType()),
                        StructField("created_at", StringType()),
                        StructField("custom_role_id", LongType()),
                        StructField("default_group_id", LongType()),
                        StructField("details", StringType()),
                        StructField("external_id", StringType()),
                        StructField("iana_time_zone", StringType()),
                        StructField("last_login_at", StringType()),
                        StructField("locale", StringType()),
                        StructField("locale_id", LongType()),
                        StructField("moderator", BooleanType()),
                        StructField("notes", StringType()),
                        StructField("only_private_comments", BooleanType()),
                        StructField("organization_id", LongType()),
                        StructField("phone", StringType()),
                        StructField("photo", MapType(StringType(), StringType())),
                        StructField("report_csv", BooleanType()),
                        StructField("restricted_agent", BooleanType()),
                        StructField("role", StringType()),
                        StructField("role_type", LongType()),
                        StructField("shared", BooleanType()),
                        StructField("shared_agent", BooleanType()),
                        StructField("shared_phone_number", BooleanType()),
                        StructField("signature", StringType()),
                        StructField("suspended", BooleanType()),
                        StructField("tags", ArrayType(StringType())),
                        StructField("ticket_restriction", StringType()),
                        StructField("time_zone", StringType()),
                        StructField("two_factor_auth_enabled", BooleanType()),
                        StructField("updated_at", StringType()),
                        StructField("user_fields", MapType(StringType(), StringType())),
                        StructField("verified", BooleanType()),
                    ]
                ),
            }

            if table_name not in schemas:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return schemas[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            """
            metadata = {
                "tickets": {"primary_key": "id", "cursor_field": "updated_at"},
                "organizations": {"primary_key": "id", "cursor_field": "updated_at"},
                "articles": {"primary_key": "id", "cursor_field": "updated_at"},
                "brands": {"primary_key": "id", "cursor_field": "updated_at"},
                "groups": {"primary_key": "id", "cursor_field": "updated_at"},
                "ticket_comments": {"primary_key": "id", "cursor_field": "updated_at"},
                "topics": {"primary_key": "id", "cursor_field": "updated_at"},
                "users": {"primary_key": "id", "cursor_field": "updated_at"},
            }

            if table_name not in metadata:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return metadata[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            # Map table names to their API endpoints and response keys
            api_config = {
                "tickets": {
                    "endpoint": "incremental/tickets.json",
                    "response_key": "tickets",
                    "supports_incremental": True,
                },
                "organizations": {
                    "endpoint": "incremental/organizations.json",
                    "response_key": "organizations",
                    "supports_incremental": True,
                },
                "articles": {
                    "endpoint": "help_center/articles.json",
                    "response_key": "articles",
                    "supports_incremental": False,
                    "supports_pagination": True,
                },
                "brands": {
                    "endpoint": "brands.json",
                    "response_key": "brands",
                    "supports_incremental": False,
                    "supports_pagination": True,
                },
                "groups": {
                    "endpoint": "groups.json",
                    "response_key": "groups",
                    "supports_incremental": False,
                    "supports_pagination": True,
                },
                "ticket_comments": {
                    "endpoint": "incremental/ticket_events.json",
                    "response_key": "ticket_events",
                    "supports_incremental": True,
                    "include": "comment_events",
                },
                "topics": {
                    "endpoint": "community/topics.json",
                    "response_key": "topics",
                    "supports_incremental": False,
                    "supports_pagination": True,
                },
                "users": {
                    "endpoint": "incremental/users.json",
                    "response_key": "users",
                    "supports_incremental": True,
                },
            }

            if table_name not in api_config:
                raise ValueError(f"Table '{table_name}' is not supported.")

            config = api_config[table_name]

            if config.get("supports_incremental", False):
                return self._read_incremental(table_name, config, start_offset)
            else:
                return self._read_paginated(table_name, config, start_offset)

        def _read_incremental(self, table_name: str, config: dict, start_offset: dict):
            """Read data from incremental API endpoints"""
            start_time = 0
            if start_offset and "start_time" in start_offset:
                start_time = start_offset["start_time"]

            endpoint = config["endpoint"]
            response_key = config["response_key"]

            # Build URL with query parameters
            url = f"{self.base_url}/{endpoint}?start_time={start_time}"
            if "include" in config:
                url += f"&include={config['include']}"

            all_records = []
            next_page = url
            last_time = start_time

            while next_page:
                resp = requests.get(next_page, headers=self.auth_header)
                if resp.status_code != 200:
                    raise Exception(
                        f"Zendesk API error for {table_name}: {resp.status_code} {resp.text}"
                    )

                data = resp.json()

                # Handle ticket_comments specially
                if table_name == "ticket_comments":
                    # Extract comments from ticket events
                    ticket_events = data.get("ticket_events", [])
                    for event in ticket_events:
                        # Create a record that combines ticket info with comments
                        if "child_events" in event:
                            for child in event["child_events"]:
                                if child.get("event_type") == "Comment":
                                    comment_record = {
                                        "id": event.get("id"),
                                        "ticket_id": event.get("ticket_id"),
                                        "created_at": event.get("created_at"),
                                        "updated_at": event.get("updated_at"),
                                        **child,
                                    }
                                    all_records.append(comment_record)
                        # Update last_time
                        try:
                            event_time = int(
                                datetime.strptime(
                                    event.get("created_at", ""), "%Y-%m-%dT%H:%M:%SZ"
                                ).timestamp()
                            )
                            if event_time > last_time:
                                last_time = event_time
                        except Exception:
                            pass
                else:
                    records = data.get(response_key, [])
                    all_records.extend(records)

                    # Update last_time based on updated_at field
                    for record in records:
                        try:
                            record_time = int(
                                datetime.strptime(
                                    record.get("updated_at", ""), "%Y-%m-%dT%H:%M:%SZ"
                                ).timestamp()
                            )
                            if record_time > last_time:
                                last_time = record_time
                        except Exception:
                            pass

                next_page = data.get("next_page")
                end_of_stream = data.get("end_of_stream", True)

                if end_of_stream or not next_page:
                    break

            return all_records, {"start_time": last_time}

        def _read_paginated(self, table_name: str, config: dict, start_offset: dict):
            """Read data from paginated API endpoints"""
            endpoint = config["endpoint"]
            response_key = config["response_key"]

            # For paginated endpoints, use page number from offset
            page = 1
            if start_offset and "page" in start_offset:
                page = start_offset["page"]

            url = f"{self.base_url}/{endpoint}?page={page}&per_page=100"

            all_records = []
            current_page = page

            while True:
                current_url = f"{self.base_url}/{endpoint}?page={current_page}&per_page=100"
                resp = requests.get(current_url, headers=self.auth_header)

                if resp.status_code != 200:
                    # Some endpoints might return 404 when no more pages
                    if resp.status_code == 404:
                        break
                    raise Exception(
                        f"Zendesk API error for {table_name}: {resp.status_code} {resp.text}"
                    )

                data = resp.json()
                records = data.get(response_key, [])

                if not records:
                    break

                all_records.extend(records)

                # Check if there's a next page
                next_page = data.get("next_page")
                if not next_page:
                    break

                current_page += 1

                # Optional: Add a reasonable limit to prevent infinite loops
                if current_page > 1000:  # Adjust as needed
                    break

            return all_records, {"page": current_page}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(self.table_name, None)

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_key", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
