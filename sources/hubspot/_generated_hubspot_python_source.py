# ==============================================================================
# Merged Lakeflow Source: hubspot
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Tuple,
)
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import random
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/hubspot/hubspot.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict) -> None:
            self.access_token = options["access_token"]
            self.base_url = "https://api.hubapi.com"
            self.auth_header = {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json",
            }
            # Cache for discovered schemas to avoid repeated API calls
            self._schema_cache = {}
            # Cache for table metadata
            self._metadata_cache = {}

            # Centralized object metadata configuration
            self._object_config = {
                "contacts": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "lastmodifieddate",
                    "associations": ["companies"],
                },
                "companies": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts"],
                },
                "deals": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "tickets"],
                },
                "tickets": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals"],
                },
                "calls": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals", "tickets"],
                },
                "emails": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals", "tickets"],
                },
                "meetings": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals", "tickets"],
                },
                "tasks": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals", "tickets"],
                },
                "notes": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": ["contacts", "companies", "deals", "tickets"],
                },
                "deal_split": {
                    "primary_key": "id",
                    "cursor_field": "updatedAt",
                    "cursor_property_field": "hs_lastmodifieddate",
                    "associations": [],
                },
            }

            # Default config for custom objects
            self._default_object_config = {
                "primary_key": "id",
                "cursor_field": "updatedAt",
                "cursor_property_field": "hs_lastmodifieddate",
                "associations": [],
            }

        def list_tables(self) -> list[str]:
            """
            List available tables including standard CRM objects and custom objects.
            """
            # Standard HubSpot CRM objects
            standard_tables = [
                "contacts",
                "companies",
                "deals",
                "tickets",
                "calls",
                "emails",
                "meetings",
                "tasks",
                "notes",
            ]

            # Add dynamic discovery of custom objects
            try:
                custom_objects = self._discover_custom_objects()
                standard_tables.extend(custom_objects)
            except Exception as e:
                print(f"Warning: Could not discover custom objects: {e}")

            return standard_tables

        def _discover_custom_objects(self) -> List[str]:
            """
            Discover custom objects from HubSpot CRM schemas API
            """
            try:
                url = f"{self.base_url}/crm/v3/schemas"
                resp = requests.get(url, headers=self.auth_header)

                if resp.status_code != 200:
                    return []

                data = resp.json()
                custom_objects = []

                # Extract custom object names
                for schema in data.get("results", []):
                    object_type = schema.get("objectTypeId", "")
                    name = schema.get("name", "")

                    # Skip standard objects, only include custom ones
                    if object_type not in ["0-1", "0-2", "0-3", "0-5"] and name:
                        custom_objects.append(name.lower())

                return custom_objects
            except Exception as e:
                print(f"Error discovering custom objects: {e}")
                return []

        def _get_object_config(self, table_name: str) -> Dict:
            """Get configuration for a specific object type"""
            return self._object_config.get(table_name, self._default_object_config)

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: The name of the table to fetch the schema for.

            Returns:
                A StructType object representing the schema of the table.
            """
            # Check cache first
            if table_name in self._schema_cache:
                return self._schema_cache[table_name]

            # Discover schema via API
            schema = self._discover_table_schema(table_name)

            # Cache the result
            self._schema_cache[table_name] = schema

            return schema

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.

            Args:
                table_name: The name of the table to fetch the metadata for.

            Returns:
                A dictionary containing the metadata of the table. It includes the following keys:
                    - primary_key: The name of the primary key of the table.
                    - cursor_field: The name of the field to use as a cursor for incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It should be one of:
                        - "snapshot": For snapshot loading.
                        - "cdc": capture incremental changes
                        - "append": incremental append
            """
            # Check cache first
            if table_name in self._metadata_cache:
                return self._metadata_cache[table_name]

            # Get metadata from object configuration
            metadata = self._get_table_metadata(table_name)

            # Cache the result
            self._metadata_cache[table_name] = metadata

            return metadata

        def _discover_table_schema(self, table_name: str) -> StructType:
            """
            Discover table schema by calling HubSpot Properties API.

            Args:
                table_name: Name of the table/object to discover schema for

            Returns:
                StructType representing the table schema
            """
            # All CRM objects follow the same schema pattern
            return self._discover_crm_object_schema(table_name)

        def _get_table_metadata(self, table_name: str) -> dict:
            """
            Get metadata for a table based on object configuration.
            """
            config = self._get_object_config(table_name)

            # Get property names and cursor property field for API calls
            properties = self._get_object_properties(table_name)
            property_names = [prop["name"] for prop in properties]

            return {
                "primary_key": config["primary_key"],
                "cursor_field": config["cursor_field"],
                "cursor_property_field": config["cursor_property_field"],
                "property_names": property_names,
                "associations": config.get("associations", []),
                "ingestion_type": "cdc",
            }

        def _discover_crm_object_schema(self, table_name: str) -> StructType:
            """
            Discover CRM object schema using HubSpot Properties API.
            Works for contacts, companies, deals, tickets, and custom objects.
            """
            # Get object configuration and properties
            config = self._get_object_config(table_name)
            properties = self._get_object_properties(table_name)

            # Build base schema fields (these are always present for CRM objects)
            base_fields = [
                StructField("id", StringType(), True),
                StructField("createdAt", StringType(), True),
                StructField("updatedAt", StringType(), True),
                StructField("archived", BooleanType(), True),
            ]

            # Add association fields based on configuration
            for association in config["associations"]:
                base_fields.append(StructField(association, ArrayType(StringType()), True))

            # Build nested properties schema based on API response
            properties_fields = []

            if isinstance(properties, list):
                for prop in properties:
                    prop_name = prop.get("name", "")
                    prop_type = prop.get("type", "string")

                    # Map HubSpot property types to Spark types
                    spark_type = self._map_hubspot_type_to_spark(prop_type)
                    properties_fields.append(StructField(prop_name, spark_type, True))

            # Create nested properties StructType
            properties_struct = StructType(properties_fields) if properties_fields else StructType([])

            # Add properties as a nested field
            base_fields.append(StructField("properties", properties_struct, True))

            schema = StructType(base_fields)

            return schema

        def _get_associations_for_object(self, table_name: str) -> List[str]:
            """Get associations to include for the given object type"""
            config = self._get_object_config(table_name)
            return config["associations"]

        def _get_object_properties(self, object_type: str) -> List[Dict]:
            """
            Fetch object properties from HubSpot Properties API
            """
            url = f"{self.base_url}/properties/v2/{object_type}/properties"

            try:
                resp = requests.get(url, headers=self.auth_header)
                if resp.status_code != 200:
                    raise Exception("API error: {resp.status_code} {resp.text}")

                return resp.json()
            except Exception as e:
                return {"error": f"Failed to get object properties: {str(e)}"}

        def _map_hubspot_type_to_spark(self, hubspot_type: str) -> DataType:
            """
            Map HubSpot property types to Spark DataTypes
            Following the requirement: strings -> StringType, integers -> LongType
            """
            type_mapping = {
                "string": StringType(),
                "enumeration": StringType(),
                "bool": BooleanType(),  # Keep boolean as boolean for better data handling
                "number": LongType(),
                "date": StringType(),  # Store as string to preserve ISO format
                "date-time": StringType(),  # Store as string to preserve ISO format
                "datetime": StringType(),
                "json": StringType(),
                "phone_number": StringType(),
                "object_coordinates": StringType(),
            }

            return type_mapping.get(hubspot_type.lower(), StringType())

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read data from HubSpot API using unified approach.

            Args:
                table_name: Name of the table to read
                start_offset: Dictionary containing cursor information for incremental reads

            Returns:
                Tuple of (records, new_offset)
            """
            # Determine if this is an incremental read
            is_incremental = (
                start_offset is not None and start_offset.get("updatedAt") is not None
            )

            if is_incremental:
                return self._read_data(table_name, start_offset, incremental=True, table_options=table_options)
            else:
                return self._read_data(table_name, None, incremental=False, table_options=table_options)

        def _read_data(
            self, table_name: str, start_offset: dict = None, incremental: bool = False,
            table_options: Dict[str, str] = None
        ):
            """Unified method to read data from HubSpot API"""

            # Get discovered properties and object configuration
            metadata = self.read_table_metadata(table_name, table_options)
            property_names = metadata.get("property_names", [])
            cursor_property_field = metadata.get("cursor_property_field")
            associations = metadata.get("associations", [])

            all_records = []
            after = None
            latest_updated = start_offset.get("updatedAt") if start_offset else None

            while True:
                if incremental:
                    # Use search API for incremental reads
                    records, after, updated_time = self._fetch_incremental_batch(
                        table_name,
                        property_names,
                        cursor_property_field,
                        start_offset,
                        after,
                    )
                    if updated_time and (
                        not latest_updated or updated_time > latest_updated
                    ):
                        latest_updated = updated_time
                else:
                    # Use objects API for full refresh
                    records, after = self._fetch_full_refresh_batch(
                        table_name, property_names, associations, after
                    )

                if not records:
                    break

                # Transform records
                transformed_records = self._transform_records(records, table_name)
                all_records.extend(transformed_records)

                # Update latest timestamp for full refresh
                if not incremental:
                    for record in transformed_records:
                        updated_at = record.get("updatedAt")
                        if updated_at and (
                            not latest_updated or updated_at > latest_updated
                        ):
                            latest_updated = updated_at

                if not after:
                    break

                # Rate limiting
                time.sleep(0.1)

            offset = {"updatedAt": latest_updated} if latest_updated else {}
            return all_records, offset

        def _fetch_full_refresh_batch(
            self,
            table_name: str,
            property_names: List[str],
            associations: List[str],
            after: str = None,
        ):
            """Fetch a batch of records using full refresh API"""
            url = f"{self.base_url}/crm/v3/objects/{table_name}?limit=100&archived=false"

            if after:
                url += f"&after={after}"
            if property_names:
                url += f"&properties={','.join(property_names)}"
            if associations:
                url += f"&associations={','.join(associations)}"

            resp = requests.get(url, headers=self.auth_header)
            if resp.status_code != 200:
                raise Exception(
                    f"HubSpot API error for {table_name}: {resp.status_code} {resp.text}"
                )

            data = resp.json()
            records = data.get("results", [])
            next_after = data.get("paging", {}).get("next", {}).get("after")

            return records, next_after

        def _fetch_incremental_batch(
            self,
            table_name: str,
            property_names: List[str],
            cursor_property_field: str,
            start_offset: dict,
            after: str = None,
        ):
            """Fetch a batch of records using incremental search API"""
            last_updated = start_offset.get("updatedAt", "1970-01-01T00:00:00.000Z")

            # Convert to milliseconds for HubSpot
            try:
                last_updated_ms = int(
                    datetime.fromisoformat(last_updated.replace("Z", "+00:00")).timestamp()
                    * 1000
                )
            except:
                last_updated_ms = 0

            search_body = {
                "filterGroups": [
                    {
                        "filters": [
                            {
                                "propertyName": cursor_property_field,
                                "operator": "GTE",
                                "value": str(last_updated_ms),
                            }
                        ]
                    }
                ],
                "sorts": [
                    {"propertyName": cursor_property_field, "direction": "ASCENDING"}
                ],
                "limit": 100,
                "properties": property_names or [],
            }

            if after:
                search_body["after"] = after

            url = f"{self.base_url}/crm/v3/objects/{table_name}/search"
            resp = requests.post(url, headers=self.auth_header, json=search_body)

            if resp.status_code != 200:
                raise Exception(
                    f"HubSpot API error for {table_name}: {resp.status_code} {resp.text}"
                )

            data = resp.json()
            records = data.get("results", [])
            next_after = data.get("paging", {}).get("next", {}).get("after")

            # Get latest update time from this batch
            latest_in_batch = last_updated
            for record in records:
                updated_at = record.get("updatedAt")
                if updated_at and updated_at > latest_in_batch:
                    latest_in_batch = updated_at

            return records, next_after, latest_in_batch

        def _transform_records(self, records: List[Dict], table_name: str) -> List[Dict]:
            """Transform HubSpot records by flattening properties and associations"""
            return [self._transform_single_record(record, table_name) for record in records]

        def _transform_single_record(self, record: Dict, table_name: str) -> Dict:
            """Transform a single HubSpot record"""
            transformed_record = {}

            # Copy base fields
            for field in ["id", "createdAt", "updatedAt", "archived", "properties"]:
                if field in record:
                    transformed_record[field] = record[field]

            # Handle associations
            transformed_record.update(self._extract_associations(record, table_name))

            return transformed_record

        def _extract_associations(self, record: Dict, table_name: str) -> Dict:
            """Extract association IDs from record"""
            associations_data = record.get("associations", {})
            config = self._get_object_config(table_name)
            expected_associations = config["associations"]
            result = {}

            for association_type in expected_associations:
                association_list = []
                if association_type in associations_data:
                    assoc_data = associations_data[association_type]
                    if isinstance(assoc_data, dict) and "results" in assoc_data:
                        association_list = [
                            item.get("id", "") for item in assoc_data["results"]
                        ]
                    elif isinstance(assoc_data, list):
                        association_list = [
                            str(item) if not isinstance(item, dict) else item.get("id", "")
                            for item in assoc_data
                        ]

                result[association_type] = association_list

            return result

        def test_connection(self) -> dict:
            """Test the connection to HubSpot API"""
            try:
                url = f"{self.base_url}/crm/v3/objects/contacts?limit=1"
                resp = requests.get(url, headers=self.auth_header)

                if resp.status_code == 200:
                    return {"status": "success", "message": "Connection successful"}
                else:
                    return {
                        "status": "error",
                        "message": f"API error: {resp.status_code} {resp.text}",
                    }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_key", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
